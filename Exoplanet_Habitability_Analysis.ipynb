{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dworlock11/Exoplanet-Machine-Learning-Analysis/blob/main/Exoplanet_Habitability_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview**"
      ],
      "metadata": {
        "id": "-jMgq7KgRaBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project intends to create machine learning models for predicting an exoplanet's type (Terran, Jovian, etc.) and mass. In addition, the most significant predictive features of these characteristics will be found and analyzed. There will be two groups of models: a classification group for predicting the type of exoplanet and a regression group for predicting the mass. Various models will be trained, tested, and evaluated, and the best model from each group will be determined. Both the performance and the time necessary for training will be considered when determining the best models.\n",
        "\n"
      ],
      "metadata": {
        "id": "DkfyrM6YRge5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Statements**"
      ],
      "metadata": {
        "id": "R2-EsQihQ4IL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgOnxV-2LYPr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, KFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import (make_scorer, confusion_matrix, ConfusionMatrixDisplay, classification_report, mean_absolute_error,\n",
        "                             root_mean_squared_error)\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from warnings import simplefilter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis and Preprocessing**"
      ],
      "metadata": {
        "id": "kZGvaz5xAdsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data from an Excel sheet is read into a DataFrame."
      ],
      "metadata": {
        "id": "v4lduI3yTMGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"Exoplanet Catalog.xlsx\")\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "gE7trk90TC4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the data can be used for model development, it needs to be cleaned and analyzed. Firstly, as many of the columns from the dataset contain a lot of null entries, it is best to simply remove the columns. All columns with more null values than a quarter of the number of rows in the dataset are removed."
      ],
      "metadata": {
        "id": "hL6MU_fg2q3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_non_null_count = df.isna().sum()\n",
        "cols_non_majority_null = col_non_null_count[col_non_null_count < len(df)/4].index.to_list()\n",
        "df = df[cols_non_majority_null]"
      ],
      "metadata": {
        "id": "nN9F605vRCHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional feature selection is conducted, as many of the features are unhelpful for model training, are copies of one another, or are highly correlated."
      ],
      "metadata": {
        "id": "JLPC33tqhIQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"P_NAME\", \"P_STATUS\", \"P_RADIUS\", \"P_YEAR\", \"P_UPDATED\", \"S_NAME\", \"S_RADIUS\", \"S_ALT_NAMES\", \"P_HABZONE_OPT\", \"P_HABZONE_CON\", \"S_CONSTELLATION_ABR\", \"P_PERIOD_ERROR_MIN\", \"P_PERIOD_ERROR_MAX\", \"S_DISTANCE_ERROR_MIN\", \"S_DISTANCE_ERROR_MAX\", \"P_FLUX_MIN\", \"P_FLUX_MAX\", \"P_TEMP_EQUIL_MIN\", \"P_TEMP_EQUIL_MAX\"], axis=1)"
      ],
      "metadata": {
        "id": "-Ll_3aw0dJyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical features with far too many unique values are removed to simplify feature encoding."
      ],
      "metadata": {
        "id": "4ba8S0Vylt40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find categorical features\n",
        "cat_features = df.select_dtypes(exclude=np.number)\n",
        "\n",
        "# Number of null values per feature\n",
        "for col in cat_features.columns:\n",
        "  print(col, \"-\", len(cat_features[col].value_counts()))\n",
        "\n",
        "# Drop features with too many null values\n",
        "df = df.drop([\"S_RA_T\", \"S_DEC_T\", \"S_CONSTELLATION\", \"S_CONSTELLATION_ENG\"], axis=1)"
      ],
      "metadata": {
        "id": "N43DQAQ0k4UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is checked for the skew of each feature to determine the appropriate imputing method for numerical data."
      ],
      "metadata": {
        "id": "3efzb0Ke3wOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.abs(df.skew(axis=0, numeric_only=True, skipna=True)).sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "xrt7HJljuQA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data is heavily skewed, the median will be chosen.\n",
        "\n",
        "The distribution of exoplanet type is observed."
      ],
      "metadata": {
        "id": "ekQnl3rbjPXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"P_TYPE\"].value_counts()"
      ],
      "metadata": {
        "id": "Xnsyp8tdhHtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single Miniterran planet can't be split amongst a training and test set. However, if the Miniterran in the data has a radius close to that of Subterrans, it would be appropriate to mask it as one."
      ],
      "metadata": {
        "id": "bXpgnEr4jUyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "miniterran = df[df[\"P_TYPE\"] == \"Miniterran\"]\n",
        "print(\"Miniterran mass:\", f\"{miniterran[\"P_RADIUS_EST\"].iloc[0]:.3f}\")\n",
        "\n",
        "subterran = df[df[\"P_TYPE\"] == \"Subterran\"]\n",
        "print(\"Smallest Subterran mass:\", f\"{subterran[\"P_RADIUS_EST\"].min():.3f}\")"
      ],
      "metadata": {
        "id": "hU1D_AdZiITd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, the radius is around 0.33 times that of Earth, which isn't too far in value from the lightest Subterran. Therefore, the planet is masked as one."
      ],
      "metadata": {
        "id": "EMxLhyGIlRIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"P_TYPE\"] = df[\"P_TYPE\"].mask(df[\"P_TYPE\"] == \"Miniterran\", \"Subterran\")\n",
        "df[\"P_TYPE\"].value_counts()"
      ],
      "metadata": {
        "id": "yKjXkBoiimkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the distribution of exoplanet mass is observed."
      ],
      "metadata": {
        "id": "Xy_05OuwU2qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"P_MASS_EST\"].describe()"
      ],
      "metadata": {
        "id": "hC6PrHIiU2Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=range(0, len(df.index)), y=df[\"P_MASS_EST\"].sort_values(ascending=False))\n",
        "plt.xlabel(\"Planet\")\n",
        "plt.ylabel(\"Mass\")\n",
        "plt.title(\"Sorted Mass of Exoplanets\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lJ7V_EnzaKzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this distribution, a couple of points should be made:\n",
        "*   The median exoplanet seems to be approximately eight times the mass of the Earth. A minority are singificantly larger. There will almost certainly be a significant difference between the RMSE and the MAE when evaluating the models, since RMSE is more sensitive to outliers.\n",
        "*   Because the smallest planets and the largest are orders of magnitude apart, it would make sense to tranform the mass into log space. This tranformed feature will be stored in a copy of the dataset.\n",
        "*   Huber loss will be used for hyperparameter training and permutation in order to balance the majority of small planets with the minority of massive ones. A manual definition is created, since no built-in one is available in the version of sci-kit learn used in this project.\n",
        "*   It's not clear what exactly it means for a planet to have a mass of 0.0. It might be a mistake. Such entries will be removed to be safe."
      ],
      "metadata": {
        "id": "KXquGmbiZKI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Huber loss\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    error = y_true - y_pred\n",
        "    abs_error = np.abs(error)\n",
        "\n",
        "    quadratic = np.minimum(abs_error, delta)\n",
        "    linear = abs_error - quadratic\n",
        "\n",
        "    return np.mean(0.5 * quadratic**2 + delta * linear)\n",
        "\n",
        "# Create scorer\n",
        "huber_scorer = make_scorer(huber_loss, greater_is_better=False, delta=1.0)\n",
        "\n",
        "# Remove entries with 0 masses\n",
        "df = df[df[\"P_MASS_EST\"] != 0.0]\n",
        "\n",
        "# Copy data and transform mass into logspace\n",
        "log_df = df.copy()\n",
        "log_df[\"Log_Mass\"] = np.log10(log_df[\"P_MASS_EST\"])\n",
        "log_df = log_df.drop(\"P_MASS_EST\", axis=1)"
      ],
      "metadata": {
        "id": "dC3QO5-XZZLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All rows where the target value is null, if any, are removed to prevent errors."
      ],
      "metadata": {
        "id": "T_Rg5LPslckk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove null entries in the type column and display number of null entries\n",
        "print(\"Number of null values in type column:\", df[\"P_TYPE\"].isna().sum())\n",
        "type_na = df[df[\"P_TYPE\"].isna()].index\n",
        "df = df.drop(type_na)\n",
        "print(\"Number of null values after removal:\", df[\"P_TYPE\"].isna().sum(), \"\\n\")\n",
        "\n",
        "# Remove null entries in the mass column and display number of null entries\n",
        "print(\"Number of null values in mass column:\", log_df[\"Log_Mass\"].isna().sum())\n",
        "mass_na = log_df[log_df[\"P_TYPE\"].isna()].index\n",
        "log_df = log_df.drop(mass_na)\n",
        "print(\"Number of null values after removal:\", log_df[\"P_TYPE\"].isna().sum())"
      ],
      "metadata": {
        "id": "eHXql9vzQeXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, model development can begin."
      ],
      "metadata": {
        "id": "rOT_oGTKl3C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exoplanet Type Classification**"
      ],
      "metadata": {
        "id": "aXnwk_kx2Cxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification models will be trained first, starting with logistic regression."
      ],
      "metadata": {
        "id": "ift3I78h2MTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "iiykhAo8LjrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is separated into the features and the target."
      ],
      "metadata": {
        "id": "e2OoXMxZbanM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"P_TYPE\", axis=1)\n",
        "y = df[\"P_TYPE\"]"
      ],
      "metadata": {
        "id": "JTPGl7TNbdEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into the training and testing data. It is stratified by the exoplanet type to make sure that a proportional number of each type is present in both the training set and the test set."
      ],
      "metadata": {
        "id": "PoOzUuQBC3Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=9)"
      ],
      "metadata": {
        "id": "G3yeJMs5b71e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data preprocessor is created, including data imputing, standardizing, and encoding."
      ],
      "metadata": {
        "id": "erZ1cs0gdos5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find numerical and categorical columns\n",
        "num_features = X_train.select_dtypes(include=np.number)\n",
        "cat_features = X_train.select_dtypes(exclude=np.number)\n",
        "num_col_names = num_features.columns\n",
        "cat_col_names = cat_features.columns\n",
        "\n",
        "# Create transformer for numerical columns\n",
        "num_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Create transformer for categorical columns\n",
        "linear_cat_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", feature_name_combiner=\"concat\"))\n",
        "])\n",
        "\n",
        "# Combine transformers\n",
        "log_preprocessor = ColumnTransformer([\n",
        "    (\"num_transformer\", num_transformer, num_col_names),\n",
        "    (\"linear_cat_transformer\", linear_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "i4tHd_DFuqqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and the hyperparameter C is tuned to prevent overfitting."
      ],
      "metadata": {
        "id": "WgfPhPBJIoge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_pipe = Pipeline([\n",
        "    (\"log_preprocessor\", log_preprocessor),\n",
        "    (\"log_reg\", LogisticRegression(\n",
        "        solver=\"lbfgs\",\n",
        "        penalty=\"l2\",\n",
        "        max_iter=300\n",
        "    ))\n",
        "])\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "param_dist = {\n",
        "    \"log_reg__C\": np.logspace(-3, 3, 15),\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(log_pipe, param_distributions=param_dist, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "bQfNnP1WLFjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is now trained and the tuned values for each hyperparameter are displayed."
      ],
      "metadata": {
        "id": "LKNJN3aJWj7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supress convergence warnings\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "print(\"C:\", f\"{search.best_params_[\"log_reg__C\"]:.3f}\")"
      ],
      "metadata": {
        "id": "AtJB1rjAWh2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is tested and evaluated."
      ],
      "metadata": {
        "id": "jG1K-_hUez1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "a3Yr8IrouQT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs very well overall. Interestingly, Subterrans performed significantly worse than the other types, likely due to a much smaller number of entries in the dataset. A confusion matrix illustrates these findings."
      ],
      "metadata": {
        "id": "OdQ-FyR6Lteq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = [\"Jovian\", \"Neptunian\", \"Subterran\", \"Superterran\", \"Terran\"]\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Logistic Regression Confusion Matrix\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ue3JFM85SLqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permutation is used to find the importance of the individual features. It will be used across all models for standardized results. The test set must be manually transformed with all preprocessing steps before implementing permutation to match the number of columns present after feature encoding."
      ],
      "metadata": {
        "id": "eJz2k95sfXSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "preprocessor = best_model.named_steps[\"log_preprocessor\"]\n",
        "log_reg = best_model.named_steps[\"log_reg\"]\n",
        "\n",
        "raw_feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Remove transformer names from features\n",
        "clean_feature_names = [\n",
        "    name.split(\"__\", 1)[1]\n",
        "    if \"__\" in name else name\n",
        "    for name in raw_feature_names\n",
        "]\n",
        "\n",
        "# Transform X_test into expanded feature space\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Run permutation importance\n",
        "importances = permutation_importance(log_reg, X_test_transformed, y_test, n_repeats=10, random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=70)\n",
        "plt.title(\"Logistic Regression Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wJRJnJQonIZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apparently, the most important feature for predicting the type of the planet is its radius. This make sense, as Jovian planets are significantly larger in size than Terrans, for example."
      ],
      "metadata": {
        "id": "1kGF9684yMhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polynomial Logistic Regression"
      ],
      "metadata": {
        "id": "olAE19ILdK2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, polynomial features are added to see if they will make a significant difference.\n",
        "\n",
        "A new preprocessor is created to accomodate polynomial features."
      ],
      "metadata": {
        "id": "OEKoLGN-dTaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build polynomial transformer\n",
        "poly_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Combine transformers\n",
        "poly_log_preprocessor = ColumnTransformer([\n",
        "    (\"poly_transformer\", poly_transformer, num_col_names),\n",
        "    (\"linear_cat_transformer\", linear_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "vYtB0dpXdK2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "sNZjPnKhdK2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_log_pipe = Pipeline([\n",
        "    (\"poly_log_preprocessor\", poly_log_preprocessor),\n",
        "    (\"log_reg\", LogisticRegression(\n",
        "        solver=\"lbfgs\",\n",
        "        penalty=\"l2\",\n",
        "        max_iter=300\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"log_reg__C\": np.logspace(-3, 3, 15),\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(poly_log_pipe, param_distributions=param_dist, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "GVce-65zdK2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is again trained and optimal regularization strength is shown."
      ],
      "metadata": {
        "id": "usBLrDqZdK2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "print(\"C:\", f\"{search.best_params_[\"log_reg__C\"]:.3f}\")"
      ],
      "metadata": {
        "id": "op-TYN3zdK2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is tested and evaluated."
      ],
      "metadata": {
        "id": "kZJP9HPmxKKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9eqbaj-rdK2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Polynomial Logistic Regression Confusion Matrix\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "31jgP2JkS49L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs around the same as without polynomial features. However, the time necessary to train is significantly longer. Therefore, there seems to be little reason to include polynomial features.\n",
        "\n",
        "Feature importance is ignored, as most of the features are simply engineered polynomial features, giving little legitimate insight."
      ],
      "metadata": {
        "id": "44pwfPrxdK2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "DRDkkz5wGiV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, a decision tree model will be trained following the same process.\n",
        "\n",
        "A new categorical transformer is created using ordinal encoding, which is suitable for tree-based models and better than one-hot encoding, since it doesn't create many sparse features."
      ],
      "metadata": {
        "id": "VFqdH5Uv-IEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create categorical transformer for tree models\n",
        "tree_cat_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
        "])\n",
        "\n",
        "# Combine transformers\n",
        "tree_preprocessor = ColumnTransformer([\n",
        "    (\"num_transformer\", num_transformer, num_col_names),\n",
        "    (\"tree_cat_transformer\", tree_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "YgB5cVXmjDwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented, testing values for the major hyperparameters of decision trees."
      ],
      "metadata": {
        "id": "bK5N2b9b-V9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_pipe = Pipeline([\n",
        "    (\"tree_preprocessor\", tree_preprocessor),\n",
        "    (\"dec_tree\", DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"dec_tree__max_depth\": [None, 2, 5, 10, 20],\n",
        "    \"dec_tree__min_samples_split\": [2, 5, 10, 20, 50],\n",
        "    \"dec_tree__min_samples_leaf\": [1, 2, 5, 10, 20],\n",
        "    \"dec_tree__max_features\": [\"sqrt\", \"log2\", None],\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(tree_pipe, param_distributions=param_dist, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "QTZIsZjD-V9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained and optimized hyperparameters are shown."
      ],
      "metadata": {
        "id": "cJ8RShBtfPk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param.split(\"__\", 1)[1], \":\" , value)"
      ],
      "metadata": {
        "id": "FXe7j2xefPk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is tested and evaluated."
      ],
      "metadata": {
        "id": "R4hkQB9YyXNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "BZd2d3rXHQ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Decision Tree Confusion Matrix\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R_L6_gQdTME1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metrics are notably better than those from the logistic regression model, especially for Subterrans. Perhaps decision trees are better suited for multiclass classification, even with little data.\n",
        "\n",
        "Permutation is once again used to discover feature importance."
      ],
      "metadata": {
        "id": "UGo257n_fPk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "preprocessor = best_model.named_steps[\"tree_preprocessor\"]\n",
        "dec_tree = best_model.named_steps[\"dec_tree\"]\n",
        "\n",
        "# Remove name of transformer from each feature\n",
        "raw_feature_names = preprocessor.get_feature_names_out()\n",
        "clean_feature_names = [\n",
        "    name.split(\"__\", 1)[1] if \"__\" in name else name\n",
        "    for name in raw_feature_names\n",
        "]\n",
        "\n",
        "# Transform X_test into expanded feature space\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Calculate importances\n",
        "importances = permutation_importance(dec_tree, X_test_transformed, y_test, n_repeats=10, random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=70)\n",
        "plt.title(\"Decision Tree Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ddH-8vqhfPk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In comparison to the logistic regression model, mass in a decision tree is a more significant predictor of exoplanet type. This may be because the logistic regression models found it more difficult to determine the importance of mass due to the nonlinear nature of the feature."
      ],
      "metadata": {
        "id": "ax3REue5H_yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "oaK_VHp0Phmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, a random forest model will be trained.\n",
        "\n",
        "The pipeline is created and hyperparameter tuning is implemented, testing ranges of values for the major hyperparameters."
      ],
      "metadata": {
        "id": "tGu3vtUCPhmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_pipe = Pipeline([\n",
        "    (\"tree_preprocessor\", tree_preprocessor),\n",
        "    (\"rand_for\", RandomForestClassifier())\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"rand_for__n_estimators\": [200, 400, 600, 800],\n",
        "    \"rand_for__max_depth\": [None, 5, 10, 20, 40],\n",
        "    \"rand_for__min_samples_split\": [2, 5, 10, 20],\n",
        "    \"rand_for__min_samples_leaf\": [1, 2, 5, 10],\n",
        "    \"rand_for__max_features\": [\"sqrt\", \"log2\", None],\n",
        "    \"rand_for__bootstrap\": [True, False],\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(forest_pipe, param_distributions=param_dist, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "I2L0QecXPhmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained and hyperparameter values are shown."
      ],
      "metadata": {
        "id": "oWtZktC2Phmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param.split(\"__\", 1)[1], \":\" , value)"
      ],
      "metadata": {
        "id": "q-VU6MKDPhmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is tested and evaluated."
      ],
      "metadata": {
        "id": "JEPKoHkA1RNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "5DVZprO8Phmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RC8AppyYTOnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metrics are around the same as those for the decision tree model. However, it takes much longer to fit, making random forests apparently unnecessary for this task.\n",
        "\n",
        "Permutation is once again used to discover feature importance."
      ],
      "metadata": {
        "id": "4UEgwA6YPhmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "preprocessor = best_model.named_steps[\"tree_preprocessor\"]\n",
        "rand_for = best_model.named_steps[\"rand_for\"]\n",
        "\n",
        "# Run permutation importance\n",
        "importances = permutation_importance(rand_for, X_test_transformed, y_test, n_repeats=10, random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=70)\n",
        "plt.title(\"Random Forest Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cVTXxbd0Phmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are similar to those from the decision tree model.\n",
        "\n",
        "This concludes the development of the classfication models."
      ],
      "metadata": {
        "id": "Q6Y4-CGpPhmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exoplanet Mass Prediction**"
      ],
      "metadata": {
        "id": "hkay5PoK4bgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, regression models will be created to predict exoplanet mass."
      ],
      "metadata": {
        "id": "A6ReEKau4g_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regression"
      ],
      "metadata": {
        "id": "_PrCnd1raZuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple linear regression model is a good starting point. Specifically, Ridge will be chosen over standard linear regression to be able to use regularization.\n",
        "\n",
        "The data is split into the features and the target."
      ],
      "metadata": {
        "id": "VGvL83QjTVsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = log_df.drop(\"Log_Mass\", axis=1)\n",
        "y = log_df[\"Log_Mass\"]"
      ],
      "metadata": {
        "id": "RNFJsujOTVsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into the training and testing data."
      ],
      "metadata": {
        "id": "pS4FisKyTVsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
      ],
      "metadata": {
        "id": "X2gemHJOTVsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformers used for the logistic regression model can be reused."
      ],
      "metadata": {
        "id": "jSkXup1xTVsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate numerical and categorical features\n",
        "num_features = X_train.select_dtypes(include=np.number)\n",
        "cat_features = X_train.select_dtypes(exclude=np.number)\n",
        "num_col_names = num_features.columns\n",
        "cat_col_names = cat_features.columns\n",
        "\n",
        "# Combine transformers\n",
        "ridge_preprocessor = ColumnTransformer([\n",
        "    (\"num_transformer\", num_transformer, num_col_names),\n",
        "    (\"linear_cat_transformer\", linear_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "jYGtSBVrTVsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "YC3yEWP8TVsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_pipe = Pipeline([\n",
        "    (\"ridge_preprocessor\", ridge_preprocessor),\n",
        "    (\"ridge\", Ridge())\n",
        "])\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "param_dist = {\n",
        "    \"ridge__alpha\": np.logspace(-4, 4)\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(ridge_pipe, param_distributions=param_dist, scoring=huber_scorer, n_iter=10, cv=kf,\n",
        "                            random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "Kz0vzL_DTVsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained and the optimized alpha value is displayed."
      ],
      "metadata": {
        "id": "o0VWc2nDTVsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "print(\"alpha:\", f\"{search.best_params_[\"ridge__alpha\"]:.3f}\")"
      ],
      "metadata": {
        "id": "7N4iPNCiTVsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE and MAE are used to evaluate the model."
      ],
      "metadata": {
        "id": "XO4y-TngjNyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = root_mean_squared_error(y_test, y_pred)\n",
        "print(f\"MAE factor  : {10**mae:.2f}×\")\n",
        "print(f\"RMSE factor : {10**rmse:.2f}×\")"
      ],
      "metadata": {
        "id": "VOzeSBzQTVsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results aren't promising. To understand why, the array of predicted values is analyzed along with the true values."
      ],
      "metadata": {
        "id": "_Dl3i5En_FUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_se = pd.Series(y_pred)\n",
        "\n",
        "y_pred_se.describe()"
      ],
      "metadata": {
        "id": "mMuFof7grPN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_se = y_test.reset_index(drop=True)\n",
        "\n",
        "y_test_se.describe()"
      ],
      "metadata": {
        "id": "8YgaRjGyw64K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There appears to be a major outlier present in the predicted values. This is further visible in a scatter plot of the actual and predicted values."
      ],
      "metadata": {
        "id": "FLzfHLQqAOZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=range(0, len(y_test)), y=10**(y_test.reset_index(drop=True)), color=\"blue\", alpha=0.4, label=\"Actual\")\n",
        "plt.scatter(x=range(0, len(y_pred)), y=10**y_pred, color=\"green\", alpha=0.4, label=\"Predicted\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Planet\")\n",
        "plt.ylabel(\"Mass\")\n",
        "plt.title(\"Actual vs Predicted Mass\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fzQH6YKYFBzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that one heavily inacurate prediction is causing the metrics to significantly worsen. Therefore, linear regression doesn't seem to be a good model for this task."
      ],
      "metadata": {
        "id": "DQkrbJafAgPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polynomial Ridge Regression"
      ],
      "metadata": {
        "id": "CEhW39HhEmL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps adding polynomial features will improve the results.\n",
        "\n",
        "A polynomial transformer is created."
      ],
      "metadata": {
        "id": "zvvpWO5XEmL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create polynomial transformer\n",
        "poly_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"poly\", PolynomialFeatures(include_bias=False)),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Combine transformers\n",
        "poly_ridge_preprocessor = ColumnTransformer([\n",
        "    (\"poly_transformer\", poly_transformer, num_col_names),\n",
        "    (\"linear_cat_transformer\", linear_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "xzklWn8PF8vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and the alpha value along with the polynomial degree are tuned."
      ],
      "metadata": {
        "id": "dOPeYfC_BONQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_ridge_pipe = Pipeline([\n",
        "    (\"poly_ridge_preprocessor\", poly_ridge_preprocessor),\n",
        "    (\"ridge\", Ridge())\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"poly_ridge_preprocessor__poly_transformer__poly__degree\" : [2, 3],\n",
        "    \"ridge__alpha\" : np.logspace(-4, 4)\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(poly_ridge_pipe, param_distributions=param_dist, scoring=huber_scorer, n_iter=10, cv=kf,\n",
        "                            random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "j0lqiFlXEmL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained and the hyperparameter values are shown."
      ],
      "metadata": {
        "id": "OUJzOxYtEmL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "print(\"alpha:\", f\"{search.best_params_[\"ridge__alpha\"]:.3f}\")\n",
        "print(\"Degree:\", search.best_params_[\"poly_ridge_preprocessor__poly_transformer__poly__degree\"])"
      ],
      "metadata": {
        "id": "CEL7V5xjEmL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE and MAE are used to evaluate the model."
      ],
      "metadata": {
        "id": "hSRvyUuSi-T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = root_mean_squared_error(y_test, y_pred)\n",
        "print(f\"MAE factor  : {10**mae:.2f}×\")\n",
        "print(f\"RMSE factor : {10**rmse:.2f}×\")"
      ],
      "metadata": {
        "id": "Q_PT_UoXEmL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are even worse than without polynomial features. In fact, the model is almost comically bad. The predicted values are again analyzed to learn why."
      ],
      "metadata": {
        "id": "OSgHFEGVB1cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_se = pd.Series(y_pred)\n",
        "\n",
        "y_pred_se.describe()"
      ],
      "metadata": {
        "id": "9vOGiC6Bxewi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_se.describe()"
      ],
      "metadata": {
        "id": "sg2Ttk_C7UBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=range(0, len(y_test)), y=10**y_test.reset_index(drop=True), color=\"blue\", alpha=0.4, label=\"Actual\")\n",
        "plt.scatter(x=range(0, len(y_pred)), y=10**y_pred, color=\"green\", alpha=0.4, label=\"Predicted\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Planet\")\n",
        "plt.ylabel(\"Mass\")\n",
        "plt.title(\"Actual vs Predicted Mass\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CRRdOXTJFWeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, there is a single inacurate value bringing the model down. With or without polynomial features, linear regression is inadequate."
      ],
      "metadata": {
        "id": "bpib4OgJCIRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree Regressor"
      ],
      "metadata": {
        "id": "aGeGi1Tr3ZgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, a decision tree is created, hopefully bearing better results.\n",
        "\n",
        "A suitable pipeline is created and important hyperparameters are tuned."
      ],
      "metadata": {
        "id": "zmG25HUvDudf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine numerical and categorical transformers\n",
        "tree_preprocessor = ColumnTransformer([\n",
        "    (\"num_transformer\", num_transformer, num_col_names),\n",
        "    (\"tree_cat_transformer\", tree_cat_transformer, cat_col_names)\n",
        "])\n",
        "\n",
        "tree_pipe = Pipeline([\n",
        "    (\"tree_preprocessor\", tree_preprocessor),\n",
        "    (\"dec_tree\", DecisionTreeRegressor())\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"dec_tree__max_depth\": [None, 3, 5, 10, 20],\n",
        "    \"dec_tree__min_samples_split\": [2, 5, 10, 20, 50],\n",
        "    \"dec_tree__min_samples_leaf\": [1, 2, 5, 10, 20, 50],\n",
        "    \"dec_tree__max_features\": [None, \"sqrt\", \"log2\"]\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(tree_pipe, param_distributions=param_dist, scoring=huber_scorer, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "B75KmyC-3U2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained and hyperparameter values are displayed."
      ],
      "metadata": {
        "id": "ygGHkX-E3U2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param.split(\"__\", 1)[1], \":\" , value)"
      ],
      "metadata": {
        "id": "2_FZXof23U2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, RMSE and MAE are used to evaluate the model."
      ],
      "metadata": {
        "id": "b11sMsD7ES02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = root_mean_squared_error(y_test, y_pred)\n",
        "print(f\"MAE factor  : {10**mae:.2f}×\")\n",
        "print(f\"RMSE factor : {10**rmse:.2f}×\")"
      ],
      "metadata": {
        "id": "ZapGUz6Vic8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results point to a strong model. Real exoplanet masses often have 30-100% observational uncertainty, so a 24% error factor is very good. This is also evident in a scatter plot."
      ],
      "metadata": {
        "id": "bZWHuUVMEf87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=range(0, len(y_test)), y=10**(y_test.reset_index(drop=True)), color=\"blue\", alpha=0.4, label=\"Actual\")\n",
        "plt.scatter(x=range(0, len(y_pred)), y=10**y_pred, color=\"green\", alpha=0.4, label=\"Predicted\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Planet\")\n",
        "plt.ylabel(\"Mass\")\n",
        "plt.title(\"Actual vs Predicted Mass\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zBKrX1Z5thAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permutation is used to find important features."
      ],
      "metadata": {
        "id": "qNnkpNsE3U2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "preprocessor = best_model.named_steps[\"tree_preprocessor\"]\n",
        "dec_tree = best_model.named_steps[\"dec_tree\"]\n",
        "\n",
        "# Remove name of transformer from each feature\n",
        "raw_feature_names = preprocessor.get_feature_names_out()\n",
        "clean_feature_names = [\n",
        "    name.split(\"__\", 1)[1] if \"__\" in name else name\n",
        "    for name in raw_feature_names\n",
        "]\n",
        "\n",
        "# Transform X_test into expanded feature space\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Run permutation importance on the classifier only\n",
        "importances = permutation_importance(dec_tree, X_test_transformed, y_test, scoring=huber_scorer, n_repeats=10,\n",
        "                                     random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=70)\n",
        "plt.title(\"Decision Tree Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UHd1LwJ049wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By far the most important features are the planet's type and radius. These findings make sense; mass and radius are often correlated and a Jovian planet will certainly have a greater mass than a Terran one."
      ],
      "metadata": {
        "id": "t9xVdLiA3U2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Regressor"
      ],
      "metadata": {
        "id": "Mkl2r0Ly7BPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps a random forest will be even better for prediction.\n",
        "\n",
        "As usual, the pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "M_C_H1Iq7BPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_pipe = Pipeline([\n",
        "    (\"tree_preprocessor\", tree_preprocessor),\n",
        "    (\"rand_for\", RandomForestRegressor())\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"rand_for__n_estimators\": [200, 300, 500, 800],\n",
        "    \"rand_for__max_depth\": [None, 5, 10, 20, 40],\n",
        "    \"rand_for__min_samples_split\": [2, 5, 10, 20],\n",
        "    \"rand_for__min_samples_leaf\": [1, 2, 5, 10, 20],\n",
        "    \"rand_for__max_features\": [\"sqrt\", \"log2\", None],\n",
        "    \"rand_for__bootstrap\": [True, False]\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(tree_pipe, param_distributions=param_dist, scoring=huber_scorer, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "TqSvea-_7BPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, the model is trained and tuned hyperparameter values are shown."
      ],
      "metadata": {
        "id": "ID5szs6N7BPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param.split(\"__\", 1)[1], \":\" , value)"
      ],
      "metadata": {
        "id": "ENsqdSjW7BPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE and MAE are used to evaluate the model as usual."
      ],
      "metadata": {
        "id": "EHWHswYE7BPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = root_mean_squared_error(y_test, y_pred)\n",
        "print(f\"MAE factor  : {10**mae:.2f}×\")\n",
        "print(f\"RMSE factor : {10**rmse:.2f}×\")"
      ],
      "metadata": {
        "id": "14K-LxUu7BPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs slightly better than the decision tree model, but takes much more time to train. These results are similar to those from the classification tree models.\n",
        "\n",
        "A plot is generated as before to underscore these findings."
      ],
      "metadata": {
        "id": "Epj46UWBGYdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=range(0, len(y_test)), y=10**(y_test.reset_index(drop=True)), color=\"blue\", alpha=0.4, label=\"Actual\")\n",
        "plt.scatter(x=range(0, len(y_pred)), y=10**y_pred, color=\"green\", alpha=0.4, label=\"Predicted\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Planet\")\n",
        "plt.ylabel(\"Mass\")\n",
        "plt.title(\"Actual vs Predicted Mass\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UnqXqH7WtkjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted and actual values are relatively close together, implying little erorr in prediction.\n",
        "\n",
        "Feature importance is analyzed."
      ],
      "metadata": {
        "id": "G_lU5jYAG9JI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "rand_for = best_model.named_steps[\"rand_for\"]\n",
        "\n",
        "# Run permutation importance on the classifier only\n",
        "importances = permutation_importance(rand_for, X_test_transformed, y_test, scoring=huber_scorer, n_repeats=10,\n",
        "                                     random_state=9, n_jobs=-1)\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=70)\n",
        "plt.title(\"Random Forest Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a3CrmqXc7BPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are similar to those from the decision tree model."
      ],
      "metadata": {
        "id": "6zj238fF7BPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "-m1ZJEM16PsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conclude, significant insights can be drawn from this analysis. Firstly, decision trees seem to be the all-around best models for both classification of exoplanet type and of mass. Random forests produce slightly better results but take longer to train. The tree models may have outperformed the linear models due to not being restricted to following a linear structure. This was especially important for predicting exoplanet mass, which doesn't follow a linear pattern. Another important discovery is that the most important features for classifying exoplanets were their radius and mass. The most important features for predicting a planet's mass were its type and radius. Therefore, exoplanet type, radius, and mass seem to be highly correlated. This has scientific precedent, as Jovian and Neptunian planets are known to have greater masses and sizes than Subterrans, for example. One can presume that these are important characteristics used by scientists when creating models with exoplanet data."
      ],
      "metadata": {
        "id": "oR8LnccsHp3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Copyright**"
      ],
      "metadata": {
        "id": "c3Bh_g2KJuqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data used in this project was taken from the exoplanet catalog found here: https://www.kaggle.com/datasets/chandrimad31/phl-exoplanet-catalog?resource=download.\n",
        "\n",
        "I claim no ownership of the data. All rights reserved to the rightful owners."
      ],
      "metadata": {
        "id": "1tqqRkhJJy4o"
      }
    }
  ]
}