{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dworlock11/Exoplanet-Machine-Learning-Analysis/blob/main/Exoplanet_Habitability_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Statements"
      ],
      "metadata": {
        "id": "R2-EsQihQ4IL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgOnxV-2LYPr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/College and Work/Exoplanet Catalog.xlsx\")\n",
        "# df = pd.read_excel(\"exoplanet_catalog.xlsx\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n9ZRjLtpQmMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "kZGvaz5xAdsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a value in the column \"P_HABITABLE\" equals \"2\", it that planet is potentially habitable under conservative estimates. For the purposes of allowing binary classification, conservative and liberal estimates will both simply be considered \"potentially habitable\"."
      ],
      "metadata": {
        "id": "RS9hOdojbd9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"P_HABITABLE\"] = df[\"P_HABITABLE\"].mask(df[\"P_HABITABLE\"] == 2, 1)\n",
        "df[\"P_HABITABLE\"].value_counts()"
      ],
      "metadata": {
        "id": "IqQH9k4CYvdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As many of the columns from the dataset contain a lot of null entries, it is best to simply remove them. All columns with the number of null values greater than a quarter the length of the dataset are removed."
      ],
      "metadata": {
        "id": "efFiyh0oAjWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_non_null_count = df.isna().sum()\n",
        "cols_non_majority_null = col_non_null_count[col_non_null_count < len(df)/4].index.to_list()\n",
        "df = df[cols_non_majority_null]"
      ],
      "metadata": {
        "id": "nN9F605vRCHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional feature selection is conducted, as many of the features are unhelpful for model training, are copies of one another, or are close in value."
      ],
      "metadata": {
        "id": "JLPC33tqhIQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"P_STATUS\", \"P_RADIUS\", \"P_YEAR\", \"P_UPDATED\", \"S_NAME\", \"S_RADIUS\", \"S_ALT_NAMES\", \"P_HABZONE_OPT\", \"P_HABZONE_CON\", \"S_CONSTELLATION_ABR\", \"P_PERIOD_ERROR_MIN\", \"P_PERIOD_ERROR_MAX\", \"S_DISTANCE_ERROR_MIN\", \"S_DISTANCE_ERROR_MAX\", \"P_FLUX_MIN\", \"P_FLUX_MAX\", \"P_TEMP_EQUIL_MIN\", \"P_TEMP_EQUIL_MAX\"], axis=1)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "-Ll_3aw0dJyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical features with far too many unique values are removed to simplify the model after encoding."
      ],
      "metadata": {
        "id": "4ba8S0Vylt40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = df.select_dtypes(include=np.number)\n",
        "cat_features = df.select_dtypes(exclude=np.number)\n",
        "\n",
        "for col in cat_features.columns:\n",
        "  print(col, \"-\", len(cat_features[col].value_counts()))\n",
        "\n",
        "df = df.drop([\"S_RA_T\", \"S_DEC_T\", \"S_CONSTELLATION\", \"S_CONSTELLATION_ENG\"], axis=1)"
      ],
      "metadata": {
        "id": "N43DQAQ0k4UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is checked for the skew of each feature to determine the appropriate imputing method. Since the data is heavily skewed, the median will be chosen."
      ],
      "metadata": {
        "id": "3efzb0Ke3wOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.skew(axis=0, numeric_only=True, skipna=True)"
      ],
      "metadata": {
        "id": "xrt7HJljuQA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "pCBUFlf13ma2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is separated into the features and the target."
      ],
      "metadata": {
        "id": "T_Rg5LPslckk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop([\"P_NAME\", \"P_HABITABLE\"], axis=1)\n",
        "y = df[\"P_HABITABLE\"]"
      ],
      "metadata": {
        "id": "0tpMgBdWwINt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All rows where the target value is null are removed."
      ],
      "metadata": {
        "id": "eZ57GriKQlqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_na = y[y.isna()]\n",
        "data = X.join(y)\n",
        "data = data.drop(y_na.index)\n",
        "X = data.drop(\"P_HABITABLE\", axis=1)\n",
        "y = data[\"P_HABITABLE\"]\n",
        "print(y.isna().sum())"
      ],
      "metadata": {
        "id": "eHXql9vzQeXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into the training and testing data."
      ],
      "metadata": {
        "id": "PoOzUuQBC3Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "G3yeJMs5b71e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the pipeline can be built, the appropriate number of components to leave after PCA must be found. An initial pipeline is created to impute null entries, as PCA doesn't accept null values. Transformers for numerical and categorical data must be created separately. Additionally, since different encoders will be used depending on the model, two different proprocessors will be built. The first will use one-hot encoding and the second ordinal encoding."
      ],
      "metadata": {
        "id": "erZ1cs0gdos5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate numerical and categorical features\n",
        "num_features = X_train.select_dtypes(include=np.number)\n",
        "cat_features = X_train.select_dtypes(exclude=np.number)\n",
        "num_col_names = num_features.columns\n",
        "cat_col_names = cat_features.columns\n",
        "\n",
        "# Build transformers\n",
        "num_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "ohe_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# Combine transformers\n",
        "combine_transformers = ColumnTransformer([\n",
        "        (\"num_transformer\", num_transformer, num_col_names),\n",
        "        (\"cat_transformer\", ohe_transformer, cat_col_names)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Build pipeline, fit to data, and plot cumulative explained variance\n",
        "component_finder = Pipeline([\n",
        "    (\"combine_transformers\", combine_transformers),\n",
        "    (\"pca\", PCA())])\n",
        "\n",
        "component_finder.fit(X_train)\n",
        "pca = component_finder.named_steps[\"pca\"]\n",
        "plt.plot(np.arange(1, pca.n_components_+1), np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")"
      ],
      "metadata": {
        "id": "i4tHd_DFuqqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because there is no clear point to cut off the number of components based on the explained variance, the number will simply be chosen based on when the cumulative variance is greater than 0.95."
      ],
      "metadata": {
        "id": "dEcpWwkyymGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_preprocessor = Pipeline([\n",
        "    (\"combine_transformers\", combine_transformers),\n",
        "    (\"pca\", PCA(n_components=0.95))\n",
        "])"
      ],
      "metadata": {
        "id": "RxZ21ssny4OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "WgfPhPBJIoge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline([\n",
        "    (\"cat_preprocessor\", cat_preprocessor),\n",
        "    (\"log_reg\", LogisticRegression())\n",
        "])\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "param_distributions = [{\n",
        "    'log_reg__penalty': ['l1', 'l2'],\n",
        "    'log_reg__C': np.logspace(-4, 4, 20),\n",
        "    'log_reg__solver': ['liblinear', 'lbfgs'],\n",
        "    'log_reg__max_iter': [200, 500, 1000],\n",
        "    'log_reg__class_weight': ['balanced', None]\n",
        "}]\n",
        "\n",
        "\n",
        "search = RandomizedSearchCV(pipe, param_distributions=param_distributions, n_iter=50, cv=kf, random_state=42)"
      ],
      "metadata": {
        "id": "bQfNnP1WLFjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained and tested and then scored with the F1 score."
      ],
      "metadata": {
        "id": "LKNJN3aJWj7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"\\n\", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "AtJB1rjAWh2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "DRDkkz5wGiV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the second preprocessor is built with OrdinalEncoder."
      ],
      "metadata": {
        "id": "n4q6PC0h4H7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Build categorical transformer\n",
        "# oe_transformer = Pipeline([\n",
        "#     (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "#     (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=11))\n",
        "# ])\n",
        "\n",
        "# # Combine transformers\n",
        "# combine_transformers = ColumnTransformer([\n",
        "#         (\"num_transformer\", num_transformer, num_col_names),\n",
        "#         (\"oe_transformer\", oe_transformer, cat_col_names)\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# # Build pipeline, fit to data, and plot cumulative explained variance\n",
        "# component_finder = Pipeline([\n",
        "#     (\"combine_transformers\", combine_transformers),\n",
        "#     (\"pca\", PCA())])\n",
        "\n",
        "# component_finder.fit(X_train)\n",
        "# pca = component_finder.named_steps[\"pca\"]\n",
        "# plt.plot(np.arange(1, pca.n_components_+1), np.cumsum(pca.explained_variance_ratio_))\n",
        "# plt.xlabel(\"Number of Components\")\n",
        "# plt.ylabel(\"Cumulative Explained Variance\")"
      ],
      "metadata": {
        "id": "S46un0VJ4MGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, because there is no clear point to cut off the number of components based on the explained variance, the number will simply be chosen based on when the cumulative variance is greater than 0.95."
      ],
      "metadata": {
        "id": "Q5VrRHW0_lDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tree_preprocessor = Pipeline([\n",
        "#     (\"combine_transformers\", combine_transformers),\n",
        "#     (\"pca\", PCA(n_components=0.95))\n",
        "# ])"
      ],
      "metadata": {
        "id": "nbXC7fa8_opx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}